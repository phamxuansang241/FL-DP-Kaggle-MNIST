{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
=======
   "execution_count": 1,
>>>>>>> 7c23767279ad7484fddb48e5bf24278085f7f918
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "'d:\\\\Linh TInh\\\\Sang RD\\\\FL-DP'"
      ]
     },
     "execution_count": 13,
=======
       "'/home/codespace/FL-DP'"
      ]
     },
     "execution_count": 1,
>>>>>>> 7c23767279ad7484fddb48e5bf24278085f7f918
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 20,
=======
   "execution_count": 2,
>>>>>>> 7c23767279ad7484fddb48e5bf24278085f7f918
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
=======
      "/home/codespace/FLDP_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
>>>>>>> 7c23767279ad7484fddb48e5bf24278085f7f918
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "from tek4fed import data_lib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
=======
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
>>>>>>> 7c23767279ad7484fddb48e5bf24278085f7f918
    "import re, string, unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import Word\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
<<<<<<< HEAD
    "import pandas as pd\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "\n",
    "seed = 2023\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
=======
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
>>>>>>> 7c23767279ad7484fddb48e5bf24278085f7f918
    "def cleaning_smsspam_dataset(df_data):\n",
    "    # covert uppercase letters to lowercase letters\n",
    "    df_data['text'] = df_data['text'].apply(lambda x: ' '.join(\n",
    "        x.lower() for x in x.split()\n",
    "    ))\n",
    "\n",
    "    # delete puctuation marks\n",
    "    df_data['text'] = df_data['text'].str.replace('[^\\w\\s]', '')\n",
    "\n",
    "    # delete numbers from texts\n",
    "    df_data['text'] = df_data['text'].str.replace('\\d', '')\n",
    "\n",
    "    # delete stopwords and punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuation = list(string.punctuation)\n",
    "    stop_words.update(punctuation)\n",
    "    df_data['text'] = df_data['text'].apply(lambda x: ' '.join(\n",
    "        x for x in x.split() if x not in stop_words\n",
    "    ))\n",
    "\n",
    "    # lemmatization and get the roots of the words\n",
    "    df_data['text'] = df_data['text'].apply(lambda x: ' '.join(\n",
    "        [Word(word).lemmatize() for word in x.split()]\n",
    "    ))\n",
    "\n",
    "    # remove words less than 3 letters\n",
    "    df_data['text'] = df_data['text'].apply(lambda x: ' '.join(\n",
    "        [x for x in x.split() if len(x) > 3]\n",
    "    ))\n",
<<<<<<< HEAD
    "\n",
=======
    "    \n",
>>>>>>> 7c23767279ad7484fddb48e5bf24278085f7f918
    "    return df_data\n",
    "\n",
    "\n",
    "def preprocessing_smsspam_dataset(datafile):\n",
    "    # load dataset\n",
<<<<<<< HEAD
    "    data = pd.read_csv(datafile, encoding='ISO-8859-1', \n",
    "                   engine='python')\n",
    "    #rename dataset columns\n",
    "    data.rename(columns = {\"v1\": \"target\", \"v2\": \"text\"}, inplace = True)\n",
    "\n",
    "    #drop unnecessary columns\n",
    "    data.drop([\"Unnamed: 2\",\"Unnamed: 3\", \"Unnamed: 4\"], axis = 1, inplace = True)\n",
    "\n",
    "    # drop duplicate data\n",
    "    data.drop_duplicates(inplace = True)\n",
    "\n",
    "    # cleaning data\n",
    "    data = cleaning_smsspam_dataset(data)\n",
    "\n",
    "    data['target'].replace({'ham': 0, 'spam': 1}, inplace=True)\n",
    "    x_data = data['text'].values\n",
    "    y_data = data['target'].values\n",
    "\n",
    "    tokenizer = get_tokenizer(tokenizer='spacy', language='en_core_web_trf')\n",
    "    tokens = [tokenizer(doc) for doc in x_data]\n",
    "    \n",
    "    vocab = build_vocab_from_iterator(tokens)\n",
    "    # label = torchtext.legacy.data.LabelField(dtype = torch.float, batch_first=True)\n",
    "    # fields = [(\"type\", label),('text', text)]\n",
    "\n",
    "    # training_data = data.TabularDataset(path=data_file, format=\"csv\", fields=fields, skip_header=True)\n",
    "    # print(dir(training_data))\n",
    "\n",
    "    print(len(vocab))\n",
    "\n",
    "    return data\n",
    "\n",
    "def smsspam_load_data(test_prob):\n",
    "    data_file = \"./datasets/smsspam/spam.csv\"\n",
    "    preprocessing_smsspam_dataset(data_file)\n",
    "    # print(dir(pre))"
=======
    "    df = pd.read_csv(datafile, encoding='ISO-8859-1', \n",
    "                   engine='python')\n",
    "    #rename dataset columns\n",
    "    df.rename(columns = {\"v1\": \"target\", \"v2\": \"text\"}, inplace = True)\n",
    "\n",
    "    #drop unnecessary columns\n",
    "    df.drop([\"Unnamed: 2\",\"Unnamed: 3\", \"Unnamed: 4\"], axis = 1, inplace = True)\n",
    "\n",
    "    # drop duplicate data\n",
    "    df.drop_duplicates(inplace = True)\n",
    "\n",
    "    # cleaning data\n",
    "    df = cleaning_smsspam_dataset(df)\n",
    "\n",
    "    df['target'].replace({'ham': 0, 'spam': 1}, inplace=True)\n",
    "\n",
    "    print(\"+++ httpparams dataset: +++\")\n",
    "    print(\"\\tNumber of normal requests: \", len(df[df['target'] == 0]))\n",
    "    print(\"\\tNumber of anomalous requests: \", len(df[df['target'] == 1]))\n",
    "    print(\"\\tNumber of total requests: \", df.shape[0])\n",
    "    return df"
>>>>>>> 7c23767279ad7484fddb48e5bf24278085f7f918
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 21,
=======
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smsspam_load_data(test_prob=0.2, max_len=71):\n",
    "    data_file = './datasets/smsspam/spam.csv'\n",
    "    data = preprocessing_smsspam_dataset(data_file)\n",
    "\n",
    "    x_data = data['text'].values\n",
    "    y_data = data['target'].values\n",
    "\n",
    "    tokenizer = get_tokenizer(tokenizer='spacy', language='en_core_web_sm')\n",
    "    \n",
    "    # print(x_data)\n",
    "    vocab = build_vocab_from_iterator([tokenizer(text) for text in x_data])\n",
    "\n",
    "    print(vocab.__len__())\n",
    "    text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "    \n",
    "    temp_x_data = [text_pipeline(text) for text in x_data]\n",
    "    temp_x_data = [np.asarray(sample, dtype=np.int32) for sample in temp_x_data]\n",
    "    temp_x_data = [np.pad(sample, (0, max(0, max_len-len(sample))), mode='constant', constant_values=0) for sample in temp_x_data]\n",
    "    temp_x_data = [sample[:max_len] for sample in temp_x_data]\n",
    "    x_data = np.asarray(temp_x_data, dtype=np.int32)\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=test_prob,\n",
    "                                                        shuffle=True, random_state=120124)\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
>>>>>>> 7c23767279ad7484fddb48e5bf24278085f7f918
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_12372\\1924160923.py:36: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_data['text'] = df_data['text'].str.replace('[^\\w\\s]', '')\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_12372\\1924160923.py:39: FutureWarning: The default value of regex will change from True to False in a future version.\n",
=======
      "/tmp/ipykernel_14752/2753061372.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_data['text'] = df_data['text'].str.replace('[^\\w\\s]', '')\n",
      "/tmp/ipykernel_14752/2753061372.py:11: FutureWarning: The default value of regex will change from True to False in a future version.\n",
>>>>>>> 7c23767279ad7484fddb48e5bf24278085f7f918
      "  df_data['text'] = df_data['text'].str.replace('\\d', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "6972\n"
     ]
    }
   ],
   "source": [
    "smsspam_load_data(0.5)"
=======
      "+++ httpparams dataset: +++\n",
      "\tNumber of normal requests:  4516\n",
      "\tNumber of anomalous requests:  653\n",
      "\tNumber of total requests:  5169\n",
      "6972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((array([[  69,   75,  292, ...,    0,    0,    0],\n",
       "         [   7,  161,  994, ...,    0,    0,    0],\n",
       "         [ 432,  198,    0, ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [ 275,  502, 2327, ...,    0,    0,    0],\n",
       "         [   2,    0,    4, ...,    0,    0,    0],\n",
       "         [ 975, 5805,  118, ...,    0,    0,    0]], dtype=int32),\n",
       "  array([0, 0, 0, ..., 0, 0, 0])),\n",
       " (array([[  87,  136,   79, ...,    0,    0,    0],\n",
       "         [ 738, 2004,  807, ...,    0,    0,    0],\n",
       "         [2017,  210,   10, ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [  19,   45, 2273, ...,    0,    0,    0],\n",
       "         [   4,  230,  207, ...,    0,    0,    0],\n",
       "         [ 854,  632,   34, ...,    0,    0,    0]], dtype=int32),\n",
       "  array([1, 0, 0, ..., 1, 0, 0])))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smsspam_load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global user.email \"hohieuocc@gmail.com\"\n",
    "!git config --global user.name \"phamxuansang\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enumerating objects: 26, done.\n",
      "Counting objects: 100% (25/25), done.\n",
      "Delta compression using up to 4 threads\n",
      "Compressing objects: 100% (14/14), done.\n",
      "Writing objects: 100% (14/14), 3.43 KiB | 1.71 MiB/s, done.\n",
      "Total 14 (delta 10), reused 0 (delta 0)\n",
      "remote: Resolving deltas: 100% (10/10), completed with 8 local objects.\u001b[K\n",
      "To https://github.com/phamxuansang241/FL-DP.git\n",
      "   92ebba6..99bb872  main -> main\n"
     ]
    }
   ],
   "source": [
    "!git push https://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@github.com/phamxuansang241/FL-DP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch's nn module has lots of useful feature\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class LSTMNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, n_layers, bidirectional):\n",
    "        \n",
    "        super(LSTMNet,self).__init__()\n",
    "        \n",
    "        # Embedding layer converts integer sequences to vector sequences\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim, padding_idx=0)\n",
    "        \n",
    "        # LSTM layer process the vector sequences \n",
    "        self.lstm = nn.LSTM(embed_dim,\n",
    "                            hidden_dim,\n",
    "                            num_layers = n_layers,\n",
    "                            bidirectional = bidirectional,\n",
    "                            dropout = 0.2,\n",
    "                            batch_first = True\n",
    "                           )\n",
    "        \n",
    "        # Dense layer to predict \n",
    "        self.fc = nn.Linear(hidden_dim * 2,output_dim)\n",
    "        # Prediction activation function\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    def forward(self,text,text_lengths):\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        # Thanks to packing, LSTM don't see padding tokens \n",
    "        # and this makes our model better\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(),batch_first=True)\n",
    "        \n",
    "        packed_output,(hidden_state,cell_state) = self.lstm(packed_embedded)\n",
    "        \n",
    "        # Concatenating the final forward and backward hidden states\n",
    "        hidden = torch.cat((hidden_state[-2,:,:], hidden_state[-1,:,:]), dim = 1)\n",
    "        \n",
    "        dense_outputs=self.fc(hidden)\n",
    "\n",
    "        #Final activation function\n",
    "        outputs=self.sigmoid(dense_outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LSTMNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m LSTMNet()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LSTMNet' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "model = LSTMNet()"
>>>>>>> 7c23767279ad7484fddb48e5bf24278085f7f918
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FLDP_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
<<<<<<< HEAD
    "hash": "feb25b4c7695d3b176bda5de41a7795a3d4786c47c38e1dae1d59a33add862d3"
=======
    "hash": "f6df391f0c1161cfbde44876906d0f22436976b227808de8fe4fc0975b5835bd"
>>>>>>> 7c23767279ad7484fddb48e5bf24278085f7f918
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
